---
title: "HW3"
author: "Nathan Krieger"
date: "2026-02-06"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

``` {r}

library(ISLR2)
#head(Carseats)

```

# Problem 1

## (a)
### (i)

```{r}

knitr::kable(head(Carseats))

m1 <- lm(Sales ~ CompPrice + Income +Advertising + Population + Price + Age + Education + Urban + US, data=Carseats)

#summary(m1)$coefficients
knitr::kable(summary(m1)$coefficients)

```



### (ii)

Null hypothesis (H0): The CompPrice of the car seat has zero effect on the sale of the car seats



Alternative hypothesis (H1): The CompPrice of the car seat has an effect on the sale of the car seats

Test statistic: 
``` {r}

comp_price_t_val <- summary(m1)$coefficients["CompPrice", "t value"]

knitr::kable(comp_price_t_val)

```

Null Distribution:

``` {r}

n <- nrow(Carseats)

null_dist <- n - 10
cat("Degrees of freedom - ", null_dist)


```

The null distribution assumes the null hypothesis is true. We can calculate the degrees of freedom to see what the distribution looks like. 400 - 9 - 1 = 390. This number indicates a distribution that is close to a normal distribution.


```{r}

comp_price_p_val <- summary(m1)$coefficients["CompPrice", "Pr(>|t|)"]

print(comp_price_p_val)

```



Since the p-value is less than $\alpha=0.05$ (2.131214e-28), we reject the null hypothesis and conclude that CompPrice has a statistically significant relationship with car seat Sales.




## (b)

I needed to assume the $\epsilon$ (errors) are normally distributed in the t-test for the regression coefficients to be valid.

## (c)

```{r}

knitr::kable(summary(m1)$sigma^2)

```

$\hat{\sigma}^2 = 3.732624$ 

This is the estimated variance of the error term. It represents the average squared deviation of the actual sales values from the predicted regression line.

More simple: It is how far away the real value is from the predicted value.

## (d)

```{r}

knitr::kable(summary(m1)$coefficients["Advertising", ])

```

Holding all other variables fixed, an increase of $1,000 in advertising is associated with an increase of approximately 0.137 thousand units (about 137 units) in expected car seat sales.

## (e)

```{r}

rss_full <- sum(resid(m1)^2)

# Reduced model (intercept only)
m0 <- lm(Sales ~ 1, data = Carseats)
rss_reduced <- sum(resid(m0)^2)

knitr::kable(rss_full)
knitr::kable(rss_reduced)

```

RSS for the full model: 1456.031

RSS for the reduced model: 3182.275

## (f)

``` {r}

knitr::kable(anova(m0, m1))

```

Null Hypothesis (H0): $\beta_1 = \beta_2 = ... = \beta_p = 0$

This means that none of the predictors are useful for predicting car seat sales.

Alternative Hypothesis (H1): At least 1 $\beta_j$ is non-zero

This means that at least 1 predictor is useful for predicting car seat sales.

Test Statistic:

F = 51.375

Null Distribution:

$F \sim F_{9, 390}$


9 is the number of predictors and 390 is the residual degrees of freedom from the full model

P-value:

P-value < 2.2e-16

Conclusion:

Since the p-value is way smaller than 0.05 we reject the null hypothesis. There is significant evidence that at least 1 of the predictors is useful when predicting car seat sales.

## (g)

``` {r}

# Create predictor values
new_X <- data.frame(
  CompPrice = mean(Carseats$CompPrice),
  Income = median(Carseats$Income),
  Advertising = 15,
  Population = 500,
  Price = 50,
  Age = 30,
  Education = 10,
  Urban = "Yes",
  US = "Yes"
)

# Confidence interval for f(X)
knitr::kable(predict(m1, newdata = new_X, interval = "confidence", level = 0.95))


```


The estimate is 15.807. 14.9 is the lower bound and 16.7 is the upper bound.

## (h)

``` {r}

# Prediction interval for Y
knitr::kable(predict(m1, newdata = new_X, interval = "prediction", level = 0.95))

```

The sources that need to be reflected in the interval are reducable and irreducable.

The estimate of 15.807 stays the same but the interval changes. 11.9 is the lower bound and 19.7 is the upper bound.

## (i)

Claim: $F(X) = E(\hat{Y})$ for fixed values of X.

Proof: 
Consider the multiple regression model:
$Y = X\beta + \epsilon$

X is the fixed design matrix
$\beta$ is the vector of regression coefficients
$\epsilon$ is the error vector satisfying

$E(\epsilon) = 0, Var(\epsilon) = \sigma^2I$

We define regression function

$f(X) = X\beta$

Taking expectations of Y which is conditional on fixed X

$E(Y | X) = E(X\beta + \epsilon | X)$

By linearity of expectation

$E(Y | X)= X\beta + E(\epsilon | X)$

But $E(\epsilon | X) = 0$

So

$E(Y | X)= X\beta = f(X)$

Since $X$ is fixed, $E(Y \mid X) = E(Y)$.

Therefore
$f(X) = E(\hat{Y})$

## (j)


``` {r}

new_X_high_price <- new_X
new_X_high_price$Price <- 450

knitr::kable(predict(m1, newdata = new_X_high_price))


```

So when the car seat sales price is $450 the predicted number of units sold is -21.16873 which is not possible because it is negative. This shows the a limitation of the model because its assuming there is a linear relationship between the price of car seats and the number sold which is not always the case. A lower bound of \$0 should be put in place because price cannot go negative.

## (k)

No. $\hat{Y}$ is not an unbiased estimator of the actual observed Y because Y includes an error term that $\hat{Y}$ cannot predict.

Assume $E(\hat{Y}) = Y$

Then, $\hat{Y} = \hat{\beta}_0 + \hat{\beta}_1 * X_1 + \cdots + \hat{\beta}_p * X_p$

But Y needs an error term

$Y = {\beta}_0 + {\beta}_1 * X_1 + \cdots + {\beta}_p * X_p + \epsilon$

So, Y has an error term and $\hat{Y}$ doesn't

Therefore

$E(\hat{Y}) \neq Y$


# Problem 2

## (a)

$m * \alpha$


## (b)

``` {r}

set.seed(123)
n = 1000  # Number of observations
p_test = c(200, 400, 500, 600, 800)
alpha = 0.05

false_positives <- c()

for (p in p_test) {
  x <- matrix(rnorm(n * p), n, p)
  y <- rnorm(n)
  
  data <- as.data.frame(x)
  
  model <- lm(y ~ ., data = data)
  
  #Step 6: Extract p-values for all predictors
  p_values <- summary(model)$coefficients[-1,4]  # use -1 to Remove intercept
  
  #Step 7: Count number of significant predictors at 0.05 level
  significant_count = sum(p_values < alpha)
  # or 
  false_positives <- c(false_positives, significant_count)
}

plot(p_test, false_positives, type="b",
     xlab = "# of tests carried out",
     ylab = "# of of false positives"
)

```

The number of false positives changing based on the amount of tests is clearly represented by the graph above. It looks like as the number of tests increase, the number of false positives increase also. However, there is a clear spike when 400 tests are carried out. In general it is safe to say that as the number of tests increase so does the number of false positives.

# Problem 3

## (a)

$\beta_0 = 2$
$\beta_1 = 3$
$\beta_2 = 5$

## (b)

```{r}

set.seed(123)

X1 = seq(0,10,length.out =100) #generates 100 equally spaced values from 0 to 10.
X2 = runif(100) #generates 100 uniform values.

epsilon = rnorm(100, mean = 0, sd = 2)

Y = 2 + 3*X1 + 5*X2 + epsilon
Y

```

## (c)


``` {r}

set.seed(123)

B <- 100
sigma2_hat <- numeric(B)

for (b in 1:B) {
  epsilon <- rnorm(100, 0, 2)
  Y <- 2 + 3*X1 + 5*X2 + epsilon
  
  model <- lm(Y ~ X1 + X2)
  sigma2_hat[b] <- summary(model)$sigma^2
}

knitr::kable(mean(sigma2_hat))

```

I am getting a mean very close to 4.

## (d)


``` {r}

hist(sigma2_hat,
     breaks = 50,
     main = expression("Distribution of " * hat(sigma)^2),
     xlab = expression(hat(sigma)^2))

abline(v = 4, col = "red", lwd = 2)


```

## (e)


An accurate estimate of $\sigma^2$ is important in multiple linear regression because it directly affects the standard errors of the regression coefficients. The standard errors are used to make confidence intervals, do hypothesis tests, and make prediction intervals.

If $\hat\sigma^2$ is not well estimated, then standard errors will be incorrect, which leads to improper interpretation. The entire statistical inference pipeline breaks down.


# Problem 4

## (a)

Setting the significance level to a = 0.05 means that before we look at the data, we are deciding that weâ€™re willing to accept a 5% chance of making a false alarm.

## (b)

I do not agree that a p-value of 0.0647 implies the predictor is not meaningful. It just means the evidence is not quite strong enough to reject the null at the chosen significance level. More thought should go into this decision than simply the p-value.

## (c)

This is a bad idea because the chances of a false positive are high. Each t-test has a 10% chance of a false positive so running 12 of these tests will result in a much higher probability than 10%:

$1 - 0.9^{12} = 0.718$

So, there is a 71.8% chance of at least 1 significant result just by chance. 

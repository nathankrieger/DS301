---
title: "HW3"
author: "Nathan Krieger"
date: "2026-02-06"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

``` {r}

library(ISLR2)
#head(Carseats)

```

# Problem 1

## (a)
### (i)

```{r}

head(Carseats)


m1 <- lm(Sales ~ CompPrice + Income +Advertising + Population + Price + Age + Education + Urban + US, data=Carseats)

summary(m1)$coefficients

```

# TODO: NO RAW R



### (ii)

Null hypothesis (H0): The CompPrice of the car seat has zero effect on the sale of the car seats



Alternative hypothesis (H1): The CompPrice of the car seat has an effect on the sale of the car seats

Test statistic: 
``` {r}

comp_price_t_val <- summary(m1)$coefficients["CompPrice", "t value"]

print(comp_price_t_val)

```

Null Distribution:

``` {r}

n <- nrow(Carseats)

null_dist <- n - 10
cat("Degrees of freedom - ", null_dist)


```

The null distribution assumes the null hypothesis is true. We can calculate the degrees of freedom to see what the distribution looks like. 400 - 9 - 1 = 390. This number indicates a distribution that is close to a normal distribution.


```{r}

comp_price_p_val <- summary(m1)$coefficients["CompPrice", "Pr(>|t|)"]

print(comp_price_p_val)

```



Since the p-value is less than $\alpha=0.05$ (2.131214e-28), we reject the null hypothesis and conclude that CompPrice has a statistically significant relationship with car seat Sales.




## (b)

I needed to assume the $\epsilon$ (errors) are normally distributed in the t-test for the regression coefficients to be valid.

## (c)

```{r}

summary(m1)$sigma^2

```

$\hat{\sigma}^2 = 3.732624$ 

This is the estimated variance of the error term. It represents the average squared deviation of the actual sales values from the predicted regression line.

## (d)

```{r}

summary(m1)$coefficients["Advertising", ]

```

This data shows that for each additional increment of $1000 spent on advertising, the expected number of car seats sold increases by around 137.

## (e)

```{r}

rss_full <- sum(resid(m1)^2)

# Reduced model (intercept only)
m0 <- lm(Sales ~ 1, data = Carseats)
rss_reduced <- sum(resid(m0)^2)

rss_full
rss_reduced

```


## (f)

``` {r}

anova(m0, m1)


```

Null Hypothesis (H0): $\beta_1 = \beta_2 = ... = \beta_p = 0$

This means that none of the predictors are useful for predicting car seat sales.

Alternative Hypothesis (H1): At least 1 $\beta_j$ is non-zero

This means that at least 1 predictor is useful for predicting car seat sales.

Test Statistic:

F = 51.375

Null Distribution:

$F \sim F_{9, 390}$


9 is the number of predictors and 390 is the residual degrees of freedom from the full model

P-value:

P-value < 2.2e-16

Conclusion:

Since the p-value is way smaller than 0.05 we reject the null hypothesis. There is significant evidence that at least 1 of the predictors is useful when predicting car seat sales.

## (g)

``` {r}

# Create predictor values
new_X <- data.frame(
  CompPrice = mean(Carseats$CompPrice),
  Income = median(Carseats$Income),
  Advertising = 15,
  Population = 500,
  Price = 50,
  Age = 30,
  Education = 10,
  Urban = "Yes",
  US = "Yes"
)

# Confidence interval for f(X)
predict(m1, newdata = new_X, interval = "confidence", level = 0.95)


```


## (h)

``` {r}

# Prediction interval for Y
predict(m1, newdata = new_X, interval = "prediction", level = 0.95)

```

## (i)
## (j)


``` {r}

new_X_high_price <- new_X
new_X_high_price$Price <- 450

predict(m1, newdata = new_X_high_price)


```

# TODO: Add words

## (k)


# Problem 2

## (a)

$m * \alpha$


## (b)

``` {r}

set.seed(123)
n = 1000  # Number of observations
p_test = c(200, 400, 500, 600, 800)
alpha = 0.05

false_positives <- c()

for (p in p_test) {
  x <- matrix(rnorm(n * p), n, p)
  y <- rnorm(n)
  
  data <- as.data.frame(x)
  
  model <- lm(y ~ ., data = data)
  
  #Step 6: Extract p-values for all predictors
  p_values <- summary(model)$coefficients[-1,4]  # use -1 to Remove intercept
  
  #Step 7: Count number of significant predictors at 0.05 level
  significant_count = sum(p_values < alpha)
  # or 
  false_positives <- c(false_positives, significant_count)
}

plot(p_test, false_positives, type="b",
     xlab = "# of tests carried out",
     ylab = "# of of false positives"
)

```

# Problem 3

## (a)

$\hat{\beta}_0 = 2$
$\hat{\beta}_1 = 3$
$\hat{\beta}_2 = 5$

## (b)

```{r}

set.seed(123)

X1 = seq(0,10,length.out =100) #generates 100 equally spaced values from 0 to 10.
X2 = runif(100) #generates 100 uniform values.

epsilon = rnorm(100, mean = 0, sd = 2)

Y = 2 + 3*X1 + 5*X2 + epsilon
Y

```

## (c)


``` {r}

set.seed(123)

B <- 1000
sigma2_hat <- numeric(B)

for (b in 1:B) {
  epsilon <- rnorm(100, sd = 2)
  Y <- 2 + 3*X1 + 5*X2 + epsilon
  
  model <- lm(Y ~ X1 + X2)
  sigma2_hat[b] <- summary(model)$sigma^2
}

mean(sigma2_hat)

```

## (d)
## (e)

# Problem 4

## (a)

Setting the significance level to a = 0.05 means that before we look at the data, we are deciding that weâ€™re willing to accept a 5% chance of making a false alarm.

## (b)

I do not agree that a p-value of 0.0647 implies the predictor is not meaningful. It just means the evidence is not quite strong enough to reject the null at the chosen significance level. More thought should go into this decision than simply the p-value.

## (c)

This is a bad idea because the chances of a false positive are high. Each t-test has a 10% chance of a false positive so running 12 of these tests will result in a much higher probability than 10%:

$1 - 0.9^{12} = 0.718$

So, there is a 71.8% chance of at least 1 significant result just by chance. 

############################################################
########################
# 1) Load the dataset  #
########################
patient <- read.table(file.choose(), header = FALSE)
############################################################
# Multiple Linear Regression (MLR): Patient Satisfaction
############################################################
########################
# 1) Load the dataset  #
########################
patient <- read.table(file.choose(), header = FALSE)
# Quick check
# Assign column names
head(patient)
names(patient) <- c("satisf", "age", "severe", "anxiety")
head(patient)
#########################
# 2) Exploratory plot
#########################
pairs(patient)
############################
# 3) Fit regression model  #
############################
# Method 1
m1 <- lm(satisf ~ age + severe + anxiety, data = patient)
# Method2 Shortcut: fit the model using ALL other variables as predictors
# m1 <- lm(satisf ~ ., data = patient)
############################################
# 4) Model output: coefficients and residuals
############################################
# Print a full summary
summary(m1)
# Show the names of components stored in the lm object
names(m1)
# Extract least squares regression coefficients
m1$coefficients
# Extract fitted values (predicted y-hat for each observation)
m1$fitted.values
# Extract residuals: observed y - fitted y
########################
# 5) Prediction examples
########################
# Predict satisfaction for a new patient with specific predictor values
m2 <- lm(satisf ~ ., data = patient)
m2$coefficents
############################################################
# In-class Activity
############################################################
# 1)Fit a linear regression model with the response as patient satisfaction
# and use all other variables as predictors. Report the least square regression coefficients.
# 2) Predicted satisfaction for observations 1, 3, and 20
# 3) Predicted satisfaction for a patient with:
#    age = 50, severe = 27, anxiety = 3
# 4) Propose a way to quantify how well our model is able to predict patient
# satisfaction (Y).
############################################################
# Group work reminder
############################################################
# Work in groups to propose a way to quantify how well the model
# predicts patient satisfaction (Y). Post relevant code + group
# member names (only one person needs to post).
#
# Post your solution to Piazza: In -class activity 1
############################################################
m2 <- lm(satisf ~ ., data = patient)
m2 <- lm(satisf ~ ., data = patient)
m2$coefficents
m2 <- lm(satisf ~ ., data = patient)
m2$coefficients
View(patient)
m2 <- lm(satisf ~ ., data = patient)
m2$coefficients
# 2) Predicted satisfaction for observations 1, 3, and 20
m2$fitted.values[c(1, 3, 20)]
# 3) Predicted satisfaction for a patient with:
#    age = 50, severe = 27, anxiety = 3
new_patient <- data.frame(age = 50, severe = 27, anxiety = 3)
predict(m2, newdata = new_patient)
summary(m2)$r.squared
knitr::opts_chunk$set(echo = TRUE)
library(tidyr)
auto <- read.table("Auto.data", header = TRUE, na.strings = "?")
nrow(auto)
ncol(auto)
auto_clean <- drop_na(auto)
nrow(auto_clean)
ncol(auto_clean)
str(auto_clean)
# auto_clean
head(auto_clean, 10)
auto_clean[c(10, 14, 29), ]
auto_clean[c(10, 14, 29), c("displacement", "horsepower")]
mean(auto_clean$mpg[auto_clean$horsepower < 200])
plot(auto_clean$horsepower, auto_clean$mpg,
xlab = "Horsepower", ylab = "MPG",
main = "MPG vs Horsepower")
boxplot(acceleration ~ year, data = auto_clean,
xlab = "Year", ylab = "Acceleration",
main = "Acceleration by Model Year")
# install.packages("ISLR2")
library(ISLR2)
head(Boston)
nrow(Boston)
ncol(Boston)
# ?Boston
mean(Boston$crim)
tapply(Boston$crim, Boston$chas, mean)
summary(Boston$crim)
Boston[Boston$crim > quantile(Boston$crim, 0.75), ]
Boston[Boston$crim >= quantile(Boston$crim, 0.98), ]
pairs(Boston[, c("crim", "lstat", "rm", "dis")])
r_vals <- sapply(Boston[, -which(names(Boston) == "crim")],
function(x) cor(x, Boston$crim))
r_vals
model_simple <- lm(crim ~ lstat, data = Boston)
summary(model_simple)
predictors <- setdiff(names(Boston), "crim")
results <- lapply(predictors, function(var) {
fit <- lm(as.formula(paste("crim ~", var)), data = Boston)
c(coef(summary(fit))[2, ], variable = var)
})
do.call(rbind, results)
plot(Boston$lstat, Boston$crim,
xlab = "Percent Lower Status (lstat)",
ylab = "Per Capita Crime Rate",
main = "Crime Rate vs Lower Status Population")
abline(lm(crim ~ lstat, data = Boston))
plot(Boston$rad, Boston$crim,
xlab = "Accessibility to Radial Highways (rad)",
ylab = "Per Capita Crime Rate",
main = "Crime Rate vs Highway Accessibility")
abline(lm(crim ~ rad, data = Boston))
plot(Boston$rm, Boston$crim,
xlab = "Average Number of Rooms (rm)",
ylab = "Per Capita Crime Rate",
main = "Crime Rate vs Number of Rooms")
abline(lm(crim ~ rm, data = Boston))
plot(Boston$medv, Boston$crim,
xlab = "Median Home Value (medv)",
ylab = "Per Capita Crime Rate",
main = "Crime Rate vs Median Home Value")
abline(lm(crim ~ medv, data = Boston))
boxplot(crim ~ chas, data = Boston,
xlab = "Bounds Charles River (chas)",
ylab = "Per Capita Crime Rate",
main = "Crime Rate by Charles River Proximity")
model_multi <- lm(crim ~ ., data = Boston)
summary(model_multi)
simple_coefs <- sapply(predictors, function(var) coef(lm(as.formula(paste("crim ~", var)), data = Boston))[2])
multi_coefs <- coef(model_multi)[predictors]
plot(simple_coefs, multi_coefs,
xlab = "Simple Regression Coefficients",
ylab = "Multiple Regression Coefficients",
main = "Coefficient Comparison")
abline(0, 1)
mean(Boston$crim[Boston$chas == 0])
mean(Boston$crim[Boston$chas == 1])
pairs(Boston[, c("crim", "lstat", "rm", "dis", "rad")])
r_vals <- sapply(Boston[, -which(names(Boston) == "crim")],
function(x) cor(x, Boston$crim))
r_vals
?nox
model_simple <- lm(crim ~ lstat, data = Boston)
summary(model_simple)
?nox
model_simple <- lm(crim ~ lstat, data = Boston)
summary(model_simple)
?auto.nox
?Boston
r_vals <- sapply(Boston[, -which(names(Boston) == "crim")],
function(x) cor(x, Boston$crim))
r_vals
predictors
predictors <- setdiff(names(Boston), "crim")
results <- lapply(predictors, function(var) {
fit <- lm(as.formula(paste("crim ~", var)), data = Boston)
c(coef(summary(fit))[2, ], variable = var)
})
do.call(rbind, results)
predictors <- setdiff(names(Boston), "crim")
# Using map/lapply logic
results_list <- lapply(predictors, function(var) {
fit <- lm(as.formula(paste("crim ~", var)), data = Boston)
# Return a data frame to keep data types (numbers as numbers)
data.frame(
variable = var,
estimate = coef(summary(fit))[2, 1],
p_value = coef(summary(fit))[2, 4]
)
})
# Combine into one clean table
simple_results_df <- do.call(rbind, results_list)
print(simple_results_df)
# Fit the multiple regression model
model_multi <- lm(crim ~ ., data = Boston)
# Extract the coefficients matrix
multi_table <- as.data.frame(summary(model_multi)$coefficients)
# Clean up column names for readability
colnames(multi_table) <- c("Estimate", "Std. Error", "t value", "Pr(>|t|)")
# Round for a professional look
round(multi_table, 4)
simple_coefs <- sapply(predictors, function(var) coef(lm(as.formula(paste("crim ~", var)), data = Boston))[2])
multi_coefs <- coef(model_multi)[predictors]
plot(simple_coefs, multi_coefs,
xlab = "Simple Regression Coefficients",
ylab = "Multiple Regression Coefficients",
main = "Coefficient Comparison")
abline(0, 1)
# 1. Collect Simple Regression Coefficients (from Part H)
# Note: simple_results_df was created in the previous step
simple_coeffs <- simple_results_df$estimate
# 2. Collect Multiple Regression Coefficients (from Part I)
# We exclude the intercept [1] to match the list of predictors
multi_coeffs <- coef(model_multi)[-1]
# 3. Create the Comparison Plot
plot(simple_coeffs, multi_coeffs,
pch = 19, col = "blue",
xlab = "Univariate Regression Coefficients",
ylab = "Multiple Regression Coefficients",
main = "Simple vs. Multiple Regression Coefficients")
# Add a 45-degree reference line
abline(0, 1, col = "red", lwd = 2, lty = 2)
# Optional: Add labels to the points to see which variables shifted most
text(simple_coeffs, multi_coeffs, labels = names(multi_coeffs),
pos = 4, cex = 0.7, col = "darkgray")
simple_coefs <- sapply(predictors, function(var) coef(lm(as.formula(paste("crim ~", var)), data = Boston))[2])
multi_coefs <- coef(model_multi)[predictors]
plot(simple_coefs, multi_coefs,
xlab = "Simple Regression Coefficients",
ylab = "Multiple Regression Coefficients",
main = "Coefficient Comparison")
abline(0, 1)
simple_coefs <- sapply(predictors, function(var) coef(lm(as.formula(paste("crim ~", var)), data = Boston))[2])
multi_coefs <- coef(model_multi)[predictors]
plot(simple_coefs, multi_coefs,
xlab = "Simple Regression Coefficients",
ylab = "Multiple Regression Coefficients",
main = "Coefficient Comparison")
abline(0, 1)
# 1. Collect Simple Regression Coefficients (from Part H)
# Note: simple_results_df was created in the previous step
simple_coeffs <- simple_results_df$estimate
# 2. Collect Multiple Regression Coefficients (from Part I)
# We exclude the intercept [1] to match the list of predictors
multi_coeffs <- coef(model_multi)[-1]
# 3. Create the Comparison Plot
plot(simple_coeffs, multi_coeffs,
pch = 19, col = "blue",
xlab = "Univariate Regression Coefficients",
ylab = "Multiple Regression Coefficients",
main = "Simple vs. Multiple Regression Coefficients")
# Add a 45-degree reference line
abline(0, 1, col = "red", lwd = 2, lty = 2)
# Optional: Add labels to the points to see which variables shifted most
text(simple_coeffs, multi_coeffs, labels = names(multi_coeffs),
pos = 4, cex = 0.7, col = "darkgray")
knitr::opts_chunk$set(echo = TRUE)
library(tidyr)
auto <- read.table("Auto.data", header = TRUE, na.strings = "?")
nrow(auto)
ncol(auto)
auto_clean <- drop_na(auto)
nrow(auto_clean)
ncol(auto_clean)
str(auto_clean)
# auto_clean
head(auto_clean, 10)
auto_clean[c(10, 14, 29), ]
auto_clean[c(10, 14, 29), c("displacement", "horsepower")]
mean(auto_clean$mpg[auto_clean$horsepower < 200])
plot(auto_clean$horsepower, auto_clean$mpg,
xlab = "Horsepower", ylab = "MPG",
main = "MPG vs Horsepower")
boxplot(acceleration ~ year, data = auto_clean,
xlab = "Year", ylab = "Acceleration",
main = "Acceleration by Model Year")
# install.packages("ISLR2")
library(ISLR2)
head(Boston)
nrow(Boston)
ncol(Boston)
# ?Boston
mean(Boston$crim)
mean(Boston$crim[Boston$chas == 0])
mean(Boston$crim[Boston$chas == 1])
summary(Boston$crim)
Boston[Boston$crim > quantile(Boston$crim, 0.75), ]
Boston[Boston$crim >= quantile(Boston$crim, 0.98), ]
pairs(Boston[, c("crim", "lstat", "rm", "dis")])
r_vals <- sapply(Boston[, -which(names(Boston) == "crim")],
function(x) cor(x, Boston$crim))
r_vals
model_simple <- lm(crim ~ lstat, data = Boston)
summary(model_simple)
predictors <- setdiff(names(Boston), "crim")
results_list <- lapply(predictors, function(var) {
fit <- lm(as.formula(paste("crim ~", var)), data = Boston)
data.frame(
variable = var,
estimate = coef(summary(fit))[2, 1],
p_value = coef(summary(fit))[2, 4]
)
})
simple_results_df <- do.call(rbind, results_list)
print(simple_results_df)
plot(Boston$lstat, Boston$crim,
xlab = "Percent Lower Status (lstat)",
ylab = "Per Capita Crime Rate",
main = "Crime Rate vs Lower Status Population")
abline(lm(crim ~ lstat, data = Boston))
plot(Boston$rad, Boston$crim,
xlab = "Accessibility to Radial Highways (rad)",
ylab = "Per Capita Crime Rate",
main = "Crime Rate vs Highway Accessibility")
abline(lm(crim ~ rad, data = Boston))
plot(Boston$rm, Boston$crim,
xlab = "Average Number of Rooms (rm)",
ylab = "Per Capita Crime Rate",
main = "Crime Rate vs Number of Rooms")
abline(lm(crim ~ rm, data = Boston))
plot(Boston$medv, Boston$crim,
xlab = "Median Home Value (medv)",
ylab = "Per Capita Crime Rate",
main = "Crime Rate vs Median Home Value")
abline(lm(crim ~ medv, data = Boston))
boxplot(crim ~ chas, data = Boston,
xlab = "Bounds Charles River (chas)",
ylab = "Per Capita Crime Rate",
main = "Crime Rate by Charles River Proximity")
model_multi <- lm(crim ~ ., data = Boston)
multi_table <- as.data.frame(summary(model_multi)$coefficients)
colnames(multi_table) <- c("Estimate", "Std. Error", "t value", "Pr(>|t|)")
round(multi_table, 4)
simple_coeffs <- simple_results_df$estimate
multi_coeffs <- coef(model_multi)[-1]
plot(simple_coeffs, multi_coeffs,
pch = 19, col = "blue",
xlab = "simple Regression Coefficients",
ylab = "Multiple Regression Coefficients",
main = "Simple vs. Multiple Regression Coefficients")
abline(0, 1, col = "red", lwd = 2, lty = 2)
text(simple_coeffs, multi_coeffs, labels = names(multi_coeffs),
pos = 4, cex = 0.7, col = "darkgray")
library(tidyverse)
knitr::opts_chunk$set(echo = TRUE)
raw_data <- read.table(file.choose(), header = TRUE)
knitr::opts_chunk$set(echo = TRUE)
#install.packages("ISLR2") #you only need to do this one time.
library(ISLR2) #you will need to do this every time you open a new R session.
head(Boston)
set.seed(1)
n <- nrow(Boston)
train_index <- sample(1:n, floor(n / 2), replace = FALSE)
train_boston <- Boston[train_index, ]
test_boston <- Boston[-train_index, ]
m1 <- lm(crim ~ ., data = train_boston)
summary(m1)
trainMSE <- sum(m1$residuals^2)/nrow(train_boston)
trainMSE
test_predict <- predict(m1, newdata = test_boston)
test_MSE <- mean((test_predict - test_boston$crim)^2)
test_MSE
set.seed(1)
n <- nrow(Boston)
train_index <- sample(1:n, floor(n / 2), replace = FALSE)
train_boston <- Boston[train_index, ]
test_boston <- Boston[-train_index, ]
m1 <- lm(crim ~ zn + indus + nox + dis + rad + ptratio + medv, data = train_boston)
summary(m1)
trainMSE <- sum(m1$residuals^2)/nrow(train_boston)
trainMSE
test_predict <- predict(m1, newdata = test_boston)
test_MSE <- mean((test_predict - test_boston$crim)^2)
test_MSE
set.seed(1) # For reproducibility
X1 = seq(0, 10, length.out = 100)
X2 = runif(100)
epsilon = rnorm(100, mean = 0, sd = 1)
Y = 2 + 3*X1 + 5*log(X2) + epsilon
Y
par(mfrow=c(1,2))
plot(X1, Y, main="X1 vs Y", col="blue", pch=19)
plot(X2, Y, main="X2 vs Y", col="darkgreen", pch=19)
n_sim <- 1000
beta1_hats <- numeric(n_sim)
for(i in 1:n_sim) {
# We generate new noise each time to simulate a new sample
eps_sim <- rnorm(100, 0, 1)
Y_sim <- 2 + 3*X1 + 5*log(X2) + eps_sim
# Fit the model (using log(X2) since that is the true relationship)
fit <- lm(Y_sim ~ X1 + log(X2))
beta1_hats[i] <- coef(fit)[2]
}
hist(beta1_hats, breaks=30, col="skyblue", main="Sampling Distribution of Beta1")
abline(v = 3, col = "red", lwd = 3) # The true Beta1
beta2_hats <- numeric(n_sim)
for(i in 1:n_sim) {
eps_sim <- rnorm(100, 0, 1)
Y_sim <- 2 + 3*X1 + 5*log(X2) + eps_sim
fit <- lm(Y_sim ~ X1 + log(X2))
beta2_hats[i] <- coef(fit)[3]
}
hist(beta2_hats, breaks=30, col="lightgreen", main="Sampling Distribution of Beta2")
abline(v = 5, col = "red", lwd = 3) # The true Beta2
knitr::opts_chunk$set(echo = TRUE)
#install.packages("ISLR2") #you only need to do this one time.
library(ISLR2) #you will need to do this every time you open a new R session.
head(Boston)
set.seed(1)
n <- nrow(Boston)
train_index <- sample(1:n, floor(n / 2), replace = FALSE)
train_boston <- Boston[train_index, ]
test_boston <- Boston[-train_index, ]
m1 <- lm(crim ~ ., data = train_boston)
summary(m1)
trainMSE <- sum(m1$residuals^2)/nrow(train_boston)
trainMSE
test_predict <- predict(m1, newdata = test_boston)
test_MSE <- mean((test_predict - test_boston$crim)^2)
test_MSE
set.seed(1)
n <- nrow(Boston)
train_index <- sample(1:n, floor(n / 2), replace = FALSE)
train_boston <- Boston[train_index, ]
test_boston <- Boston[-train_index, ]
m1 <- lm(crim ~ zn + indus + nox + dis + rad + ptratio + medv, data = train_boston)
summary(m1)
trainMSE <- sum(m1$residuals^2)/nrow(train_boston)
trainMSE
test_predict <- predict(m1, newdata = test_boston)
test_MSE <- mean((test_predict - test_boston$crim)^2)
test_MSE
set.seed(1) # For reproducibility
X1 = seq(0, 10, length.out = 100)
X2 = runif(100)
epsilon = rnorm(100, mean = 0, sd = 1)
Y = 2 + 3*X1 + 5*log(X2) + epsilon
Y
par(mfrow=c(1,2))
plot(X1, Y, main="X1 vs Y", col="blue", pch=19)
plot(X2, Y, main="X2 vs Y", col="darkgreen", pch=19)
n_sim <- 10000
beta1_hats <- numeric(n_sim)
for(i in 1:n_sim) {
# We generate new noise each time to simulate a new sample
eps_sim <- rnorm(100, 0, 1)
Y_sim <- 2 + 3*X1 + 5*log(X2) + eps_sim
# Fit the model (using log(X2) since that is the true relationship)
fit <- lm(Y_sim ~ X1 + log(X2))
beta1_hats[i] <- coef(fit)[2]
}
hist(beta1_hats, breaks=30, col="skyblue", main="Sampling Distribution of Beta1")
abline(v = 3, col = "red", lwd = 3) # The true Beta1
beta2_hats <- numeric(n_sim)
for(i in 1:n_sim) {
eps_sim <- rnorm(100, 0, 1)
Y_sim <- 2 + 3*X1 + 5*log(X2) + eps_sim
fit <- lm(Y_sim ~ X1 + log(X2))
beta2_hats[i] <- coef(fit)[3]
}
hist(beta2_hats, breaks=30, col="lightgreen", main="Sampling Distribution of Beta2")
abline(v = 5, col = "red", lwd = 3) # The true Beta2
n_sim <- 10000
beta1_hats <- numeric(n_sim)
for(i in 1:n_sim) {
# We generate new noise each time to simulate a new sample
eps_sim <- rnorm(100, 0, 1)
Y_sim <- 2 + 3*X1 + 5*log(X2) + eps_sim
# Fit the model (using log(X2) since that is the true relationship)
fit <- lm(Y_sim ~ X1 + log(X2))
beta1_hats[i] <- coef(fit)[2]
}
mean(beta1_hats)
beta2_hats <- numeric(n_sim)
for(i in 1:n_sim) {
eps_sim <- rnorm(100, 0, 1)
Y_sim <- 2 + 3*X1 + 5*log(X2) + eps_sim
fit <- lm(Y_sim ~ X1 + log(X2))
beta2_hats[i] <- coef(fit)[3]
}
mean(beta2_hats)
beta2_hats <- rep(NA, n_sim)
for(i in 1:n_sim) {
eps_sim <- rnorm(100, 0, 1)
Y_sim <- 2 + 3*X1 + 5*log(X2) + eps_sim
fit <- lm(Y_sim ~ X1 + log(X2))
beta2_hats[i] <- coef(fit)[3]
}
mean(beta2_hats)
n_sim <- 10000
beta1_hats <- rep(NA, n_sim)
for(i in 1:n_sim) {
# We generate new noise each time to simulate a new sample
eps_sim <- rnorm(100, 0, 1)
Y_sim <- 2 + 3*X1 + 5*log(X2) + eps_sim
# Fit the model (using log(X2) since that is the true relationship)
fit <- lm(Y_sim ~ X1 + log(X2))
beta1_hats[i] <- coef(fit)[2]
}
mean(beta1_hats)
n_sim <- 10000
beta1_hats <- rep(NA, n_sim)
for(i in 1:n_sim) {
# We generate new noise each time to simulate a new sample
eps_sim <- rnorm(100, 0, 1)
Y_sim <- 2 + 3*X1 + 5*log(X2) + eps_sim
# Fit the model (using log(X2) since that is the true relationship)
fit <- lm(Y_sim ~ X1 + log(X2))
beta1_hats[i] <- coef(fit)[2]
}
mean(beta1_hats)

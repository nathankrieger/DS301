---
title: "Untitled"
author: "Nathan Krieger"
date: "2026-02-18"
output: pdf_document
---

# K folds with varying degrees - REAL DATA 

```{r}

# Predicting medv based on lstat

set.seed(123)


library(ISLR2)
library(caret)
library(ggplot2)
#install.packages('caret')

k = 5 # Number of folds

max_degree <- 9
errors <- numeric(max_degree)

# Create folds
flds <- createFolds(Boston$medv, k, list = TRUE)
flds[[1]]

for (d in 1:max_degree) {
  
  folds <- numeric(k)
                   
  for(i in 1:k){
    # Get test indices for this fold
    test_index = flds[[i]]
    test = Boston[test_index, ]
    train = Boston[-test_index, ]
    
    m <- lm(medv ~ poly(lstat, d, raw = TRUE), data = train)
    
    predict <- predict(m, newdata = test)
    
    folds[i] = mean((test$medv - predict)^2)
    
  }
  
  errors[d] = mean(folds)
}

cv_results <- data.frame(Degree = 1:max_degree, CV_Error = errors)
print(cv_results)

ggplot(cv_results, aes(x = Degree, y = CV_Error)) +
  geom_line(color = "steelblue", size = 1) +
  geom_point(color = "darkred", size = 2) +
  scale_x_continuous(breaks = 1:9) +
  labs(title = "5-Fold CV Error vs. Polynomial Degree",
       x = "Polynomial Degree",
       y = "Mean Squared Error (CV)") +
  theme_minimal()


```


# K folds with varying degrees - FAKE DATA 

```{r}

set.seed(123)

n <- 500

X1 <- seq(0, 5, length.out = n)

beta_0 <- 1
beta_1 <- 1
beta_2 <- 2
beta_3 <- 3

error = rnorm(n,0,1)

y <- beta_0 + beta_1 * X1 + beta_2 * X1^2 + beta_3 * X1^3 + error

df <- data.frame(X1, y)

library(ISLR2)
library(caret)
library(ggplot2)
#install.packages('caret')

k = 5 # Number of folds

max_degree <- 9
errors <- numeric(max_degree)

# Create folds
flds <- createFolds(df$y, k, list = TRUE)
#flds[[1]]

for (d in 1:max_degree) {
  
  folds <- numeric(k)
                   
  for(i in 1:k){
    # Get test indices for this fold
    test_index = flds[[i]]
    test = df[test_index, ]
    train = df[-test_index, ]
    
    m <- lm(y ~ poly(X1, d, raw = TRUE), data = train)
    
    predict <- predict(m, newdata = test)
    
    folds[i] = mean((test$y - predict)^2)
    
  }
  
  errors[d] = mean(folds)
}

cv_results <- data.frame(Degree = 1:max_degree, CV_Error = errors)
print(cv_results)

ggplot(cv_results, aes(x = Degree, y = CV_Error)) +
  geom_line(color = "steelblue", size = 1) +
  geom_point(color = "darkred", size = 2) +
  scale_x_continuous(breaks = 1:9) +
  labs(title = "5-Fold CV Error vs. Polynomial Degree",
       x = "Polynomial Degree",
       y = "Mean Squared Error (CV)") +
  theme_minimal()


```


# LOOCV with varying degrees - FAKE DATA

```{r}


set.seed(123)

n <- 500

X1 <- seq(0, 5, length.out = n)

beta_0 <- 1
beta_1 <- 1
beta_2 <- 2
beta_3 <- 3

error = rnorm(n,0,1)

y <- beta_0 + beta_1 * X1 + beta_2 * X1^2 + beta_3 * X1^3 + error

df <- data.frame(X1, y)

max_degree <- 5
errors <- numeric(max_degree)

for (d in 1:max_degree) {
  
  abc <- numeric(n)
                   
  for(i in 1:n){
    test = df[i, ]
    train = df[-i, ]
    
    m <- lm(y ~ poly(X1, d, raw = TRUE), data = train)
    
    predict <- predict(m, newdata = test)
    
    abc[i] = mean((test$y - predict)^2)
    
  }
  
  errors[d] = mean(abc)
}

cv_results <- data.frame(Degree = 1:max_degree, CV_Error = errors)
print(cv_results)

ggplot(cv_results, aes(x = Degree, y = CV_Error)) +
  geom_line(color = "steelblue", size = 1) +
  geom_point(color = "darkred", size = 2) +
  scale_x_continuous(breaks = 1:9) +
  labs(title = "LOOCV Error vs. Polynomial Degree",
       x = "Polynomial Degree",
       y = "Mean Squared Error (CV)") +
  theme_minimal()


```
  
  
# LOOCV with varying degrees - REAL DATA

```{r}

library(ISLR2)
library(ggplot2)

max_degree <- 5
errors <- numeric(max_degree)

n <- nrow(Boston)

for (d in 1:max_degree) {
  
  abc <- numeric(n)
                   
  for(i in 1:n){
    test = Boston[i, ]
    train = Boston[-i, ]
    
    m <- lm(medv ~ poly(lstat, d, raw = TRUE), data = train)
    
    predict <- predict(m, newdata = test)
    
    abc[i] = mean((test$medv - predict)^2)
    
  }
  
  errors[d] = mean(abc)
}

cv_results <- data.frame(Degree = 1:max_degree, CV_Error = errors)
print(cv_results)

ggplot(cv_results, aes(x = Degree, y = CV_Error)) +
  geom_line(color = "steelblue", size = 1) +
  geom_point(color = "darkred", size = 2) +
  scale_x_continuous(breaks = 1:9) +
  labs(title = "LOOCV Error vs. Polynomial Degree",
       x = "Polynomial Degree",
       y = "Mean Squared Error (CV)") +
  theme_minimal()


```



# TESTING if sigma hat squared is an unbiased estimator


```{r}

set.seed(123)

X1 = seq(0,10,length.out =100) #generates 100 equally spaced values from 0 to 10.
X2 = runif(100) #generates 100 uniform values.

epsilon = rnorm(100, mean = 0, sd = 2)

Y = 2 + 3*X1 + 5*X2 + epsilon

set.seed(123)

B <- 100
sigma2_hat <- numeric(B)

for (b in 1:B) {
  epsilon <- rnorm(100, 0, 2)
  Y <- 2 + 3*X1 + 5*X2 + epsilon
  
  model <- lm(Y ~ X1 + X2)
  sigma2_hat[b] <- summary(model)$sigma^2
}

knitr::kable(mean(sigma2_hat))

```


